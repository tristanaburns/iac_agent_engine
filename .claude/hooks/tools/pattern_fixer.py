#!/usr/bin/env python3
"""
Pattern Fixer.

Automatically fixes specific types of forbidden pattern violations using
the JSON report generated by pattern_scanner.py.

This tool provides safe, automated fixes for well-defined pattern violations
while creating backups of all modified files.

Author: System
Version: 1.0.0
"""

import argparse
import json
import logging
import os
import re
import shutil
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PatternFixer:
    """Applies automatic fixes for forbidden pattern violations."""

    def __init__(self, violations_file: str, backup_dir: str = "backups"):
        """
        Initialize pattern fixer.

        Args:
            violations_file: Path to violations JSON report
            backup_dir: Directory to store file backups
        """
        self.violations_file = violations_file
        self.backup_dir = Path(backup_dir)
        self.violations_data: Dict[str, Any] = {}
        self.fixes_applied: List[Dict[str, Any]] = []
        self.files_modified: Set[str] = set()

        # Ensure backup directory exists
        self.backup_dir.mkdir(parents=True, exist_ok=True)

    def load_violations(self) -> bool:
        """Load violations data from JSON report."""
        try:
            with open(self.violations_file, "r", encoding="utf-8") as f:
                self.violations_data = json.load(f)
            total_violations = self.violations_data.get("total_violations", 0)
            logger.info(f"Loaded violations report with {total_violations} violations")
            return True
        except FileNotFoundError:
            logger.error(f"Violations file not found: {self.violations_file}")
            return False
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in violations file: {e}")
            return False
        except Exception as e:
            logger.error(f"Error loading violations: {e}")
            return False

    def create_backup(self, file_path: str) -> str:
        """
        Create backup of file before modification.

        Args:
            file_path: Path to file to backup

        Returns:
            Path to backup file
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path_obj = Path(file_path)

        # Create backup path preserving directory structure
        relative_path = (
            file_path_obj.relative_to(Path.cwd())
            if file_path_obj.is_absolute()
            else file_path_obj
        )
        backup_path = (
            self.backup_dir
            / f"{timestamp}_{relative_path.as_posix().replace('/', '_')}"
        )

        try:
            shutil.copy2(file_path, backup_path)
            logger.debug(f"Created backup: {backup_path}")
            return str(backup_path)
        except Exception as e:
            logger.error(f"Failed to create backup for {file_path}: {e}")
            raise

    def _validate_violations_data(self) -> bool:
        """
        Validate that violations data is loaded and accessible.

        Returns:
            True if violations data is valid, False otherwise
        """
        logger.debug("Starting violations data validation")

        if not self.violations_data:
            logger.error("No violations data loaded")
            logger.debug("Violations data validation failed: data is empty")
            return False

        logger.debug("Violations data validation successful")
        return True

    def _filter_print_violations(self) -> List[Dict[str, Any]]:
        """
        Filter violations to find print statement violations with comprehensive logging.

        Returns:
            List of print statement violations
        """
        logger.debug("Starting print violations filtering")

        print_violations: List[Dict[str, Any]] = []
        all_violations = self.violations_data.get("all_violations", [])
        logger.debug(f"Processing {len(all_violations)} total violations")

        for violation in all_violations:
            pattern = violation.get("pattern")
            scope = violation.get("scope")
            logger.debug(f"Checking violation: pattern='{pattern}', scope='{scope}'")

            if pattern == "print\\(" and scope == "code_block":
                print_violations.append(violation)
                logger.debug(
                    f"Print violation found: file={violation.get('file')}, "
                    f"line={violation.get('line_number')}"
                )

        logger.debug(
            f"Filtering complete: found {len(print_violations)} print violations"
        )

        if not print_violations:
            logger.info("No print statement violations found")
            return []

        logger.info(f"Found {len(print_violations)} print statement violations")
        return print_violations

    def _group_violations_by_file(
        self, print_violations: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Group print violations by file path with comprehensive logging.

        Args:
            print_violations: List of print statement violations

        Returns:
            Dictionary mapping file paths to their violations
        """
        logger.debug("Starting violation grouping by file")

        violations_by_file: Dict[str, List[Dict[str, Any]]] = {}
        for violation in print_violations:
            file_path = violation["file"]
            logger.debug(f"Processing violation for file: {file_path}")

            if file_path not in violations_by_file:
                violations_by_file[file_path] = []
                logger.debug(f"Created new group for file: {file_path}")

            violations_by_file[file_path].append(violation)

        logger.info(f"Grouped violations into {len(violations_by_file)} files")
        for file_path, file_violations in violations_by_file.items():
            logger.info(f"File {file_path}: {len(file_violations)} violations")

        return violations_by_file

    def _process_file_violations(
        self, file_path: str, file_violations: List[Dict[str, Any]]
    ) -> bool:
        """
        Process violations for a single file with comprehensive error handling.

        Args:
            file_path: Path to the file to process
            file_violations: List of violations for this file

        Returns:
            True if file was successfully processed and modified, False otherwise
        """
        logger.debug(f"Starting file processing for: {file_path}")
        logger.debug(
            f"Processing {len(file_violations)} violations for file: {file_path}"
        )

        try:
            # Create backup with logging
            logger.debug(f"Creating backup for file: {file_path}")
            backup_path = self.create_backup(file_path)
            logger.debug(f"Backup created successfully: {backup_path}")

            # Read file content with logging
            logger.debug(f"Reading file content: {file_path}")
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")
            logger.debug(f"File content read successfully, {len(lines)} lines")

            # Track modifications
            modified = False

            # Process violations in reverse order to maintain line numbers
            logger.debug("Sorting violations in reverse order by line number")
            sorted_violations = sorted(
                file_violations, key=lambda x: x["line_number"], reverse=True
            )
            logger.debug(f"Processing {len(sorted_violations)} sorted violations")

            for violation in sorted_violations:
                line_num = violation["line_number"] - 1  # Convert to 0-based index
                logger.debug(
                    f"Processing violation at line {violation['line_number']} (index {line_num})"
                )

                if 0 <= line_num < len(lines):
                    original_line = lines[line_num]
                    logger.debug(f"Original line: {original_line.strip()}")

                    # Convert print to logger statement
                    fixed_line = self._convert_print_to_logging(original_line)
                    if fixed_line != original_line:
                        lines[line_num] = fixed_line
                        modified = True
                        logger.debug(f"Line modified: {fixed_line.strip()}")

                        self.fixes_applied.append(
                            {
                                "file": file_path,
                                "line_number": violation["line_number"],
                                "original": original_line.strip(),
                                "fixed": fixed_line.strip(),
                                "fix_type": "print_to_logging",
                                "backup_path": backup_path,
                            }
                        )
                    else:
                        logger.debug("No modification needed for this line")
                else:
                    logger.warning(
                        f"Line number {line_num} out of range for file {file_path}"
                    )

            # Add logging import if needed and modifications were made
            if modified:
                logger.debug("File was modified, checking for logging import")
                if not self._has_logging_import(lines):
                    logger.debug("No logging import found, adding import")
                    # Find appropriate place to add import
                    import_line = self._find_import_insertion_point(lines)
                    lines.insert(import_line, "import logging")
                    logger.debug(f"Added logging import at line {import_line + 1}")

                    self.fixes_applied.append(
                        {
                            "file": file_path,
                            "line_number": import_line + 1,
                            "original": "",
                            "fixed": "import logging",
                            "fix_type": "add_logging_import",
                            "backup_path": backup_path,
                        }
                    )
                else:
                    logger.debug("Logging import already exists")

                # Write modified content back
                logger.debug(f"Writing modified content back to file: {file_path}")
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write("\n".join(lines))

                self.files_modified.add(file_path)
                logger.info(f"Fixed print statements in: {file_path}")
                logger.debug(f"File processing completed successfully: {file_path}")
                return True
            else:
                logger.debug(f"No modifications made to file: {file_path}")
                return False

        except Exception as e:
            logger.error(f"Error fixing print statements in {file_path}: {e}")
            logger.debug(f"File processing failed for: {file_path}", exc_info=True)
            return False

    def fix_print_statements(self) -> int:
        """
        Fix print() statements by converting them to logging statements.

        Returns:
            Number of files fixed
        """
        logger.debug("Starting fix_print_statements execution")

        # Phase 1: Validate violations data
        if not self._validate_violations_data():
            return 0

        # Phase 1: Filter print violations with comprehensive logging
        print_violations = self._filter_print_violations()
        if not print_violations:
            return 0

        # Phase 2: Group violations by file with comprehensive logging
        violations_by_file = self._group_violations_by_file(print_violations)

        # Phase 3: Process violations for each file with orchestrated error handling
        files_fixed = 0
        for file_path, file_violations in violations_by_file.items():
            if self._process_file_violations(file_path, file_violations):
                files_fixed += 1

        logger.info(f"Fixed print statements in {files_fixed} files")
        return files_fixed

    def _convert_print_to_logging(self, line: str) -> str:
        """Convert a print statement to a logging statement."""
        # Pattern to match print statements
        print_pattern = re.compile(r"(\s*)print\s*\(\s*(.*?)\s*\)\s*$", re.DOTALL)
        match = print_pattern.match(line)

        if not match:
            return line

        indent = match.group(1)
        print_content = match.group(2)

        # Use logger.info for most cases
        # This is a simple conversion - more sophisticated logic could be added
        return f"{indent}logger.info({print_content})"

    def _has_logging_import(self, lines: List[str]) -> bool:
        """Check if logging import already exists."""
        for line in lines:
            if re.match(r"^\s*import\s+logging\s*$", line) or re.match(
                r"^\s*from\s+logging\s+import", line
            ):
                return True
        return False

    def _find_import_insertion_point(self, lines: List[str]) -> int:
        """Find appropriate line to insert logging import."""
        import_end = 0
        docstring_state = self._initialize_docstring_state()

        for i, line in enumerate(lines):
            stripped = line.strip()

            # Handle docstrings
            if self._process_docstring_line(stripped, docstring_state):
                continue

            # Skip comments and empty lines at the top
            if self._should_skip_line(stripped):
                continue

            # Process import lines
            import_end = self._process_import_line(stripped, i, import_end)
            if import_end > 0 and not self._is_import_line(stripped):
                # We've passed the import section
                break

        return import_end

    def _initialize_docstring_state(self) -> Dict[str, Any]:
        """Initialize docstring tracking state.

        Returns:
            Dictionary containing docstring state
        """
        return {"in_docstring": False, "docstring_quotes": None}

    def _process_docstring_line(self, stripped: str, state: Dict[str, Any]) -> bool:
        """Process a line for docstring handling.

        Args:
            stripped: Stripped line content
            state: Docstring state dictionary

        Returns:
            True if line should be skipped, False otherwise
        """
        if not state["in_docstring"]:
            if stripped.startswith('"""') or stripped.startswith("'''"):
                state["in_docstring"] = True
                state["docstring_quotes"] = stripped[:3]
                if stripped.count(state["docstring_quotes"]) >= 2:
                    state["in_docstring"] = False  # Single line docstring
                return True
        else:
            if state["docstring_quotes"] and state["docstring_quotes"] in stripped:
                state["in_docstring"] = False
            return True

        return False

    def _should_skip_line(self, stripped: str) -> bool:
        """Check if line should be skipped during import analysis.

        Args:
            stripped: Stripped line content

        Returns:
            True if line should be skipped, False otherwise
        """
        return stripped.startswith("#") or not stripped

    def _is_import_line(self, stripped: str) -> bool:
        """Check if line is an import statement.

        Args:
            stripped: Stripped line content

        Returns:
            True if line is an import, False otherwise
        """
        return stripped.startswith("import ") or stripped.startswith("from ")

    def _process_import_line(
        self, stripped: str, line_index: int, import_end: int
    ) -> int:
        """Process an import line and update import_end.

        Args:
            stripped: Stripped line content
            line_index: Current line index
            import_end: Current import end position

        Returns:
            Updated import end position
        """
        if self._is_import_line(stripped):
            return line_index + 1
        return import_end

    def _process_advanced_violations(self) -> Optional[Dict[str, List[Dict[str, Any]]]]:
        """Process and group advanced pattern violations with comprehensive logging."""
        logger.debug("Starting advanced violations processing")

        if not self.violations_data:
            logger.error("No violations data loaded")
            logger.debug("Violations data validation failed")
            return None

        logger.debug("Filtering advanced pattern violations")
        advanced_violations: List[Dict[str, Any]] = []
        for violation in self.violations_data.get("all_violations", []):
            if violation.get("pattern") == "advanced":
                advanced_violations.append(violation)

        if not advanced_violations:
            logger.info("No 'advanced' pattern violations found")
            logger.debug("No advanced violations to process")
            return None

        logger.info(f"Found {len(advanced_violations)} 'advanced' pattern violations")
        logger.debug(f"Processing {len(advanced_violations)} advanced violations")

        # Group by file to handle filename changes
        logger.debug("Grouping violations by file")
        violations_by_file: Dict[str, List[Dict[str, Any]]] = {}
        for violation in advanced_violations:
            file_path = violation["file"]
            if file_path not in violations_by_file:
                violations_by_file[file_path] = []
            violations_by_file[file_path].append(violation)

        logger.debug(f"Grouped violations into {len(violations_by_file)} files")
        for file_path, file_violations in violations_by_file.items():
            logger.debug(f"File {file_path}: {len(file_violations)} violations")

        return violations_by_file

    def _handle_filename_violations(
        self, file_path: str, file_violations: List[Dict[str, Any]]
    ) -> Tuple[int, Optional[str]]:
        """Handle filename violations by renaming files with comprehensive logging."""
        logger.debug(f"Starting filename violation handling for {file_path}")

        # Check if this is a filename violation
        filename_violations = [v for v in file_violations if v.get("scope") == "file"]

        if not filename_violations:
            logger.debug(f"No filename violations found for {file_path}")
            return 0, file_path

        logger.debug(
            f"Found {len(filename_violations)} filename violations for {file_path}"
        )

        # Handle filename renaming
        old_path = Path(file_path)
        new_filename = old_path.name.replace("advanced", "sophisticated").replace(
            "Advanced", "Sophisticated"
        )
        new_path = old_path.parent / new_filename

        logger.debug("Generated new filename: %s -> %s", old_path.name, new_filename)

        if new_path != old_path and not new_path.exists():
            logger.debug(f"Proceeding with rename: {old_path} -> {new_path}")

            # Create backup
            backup_path = self.create_backup(file_path)
            logger.debug(f"Backup created: {backup_path}")

            # Rename file
            os.rename(file_path, new_path)
            self.files_modified.add(str(new_path))

            self.fixes_applied.append(
                {
                    "file": file_path,
                    "line_number": 0,
                    "original": str(old_path),
                    "fixed": str(new_path),
                    "fix_type": "rename_file_advanced",
                    "backup_path": backup_path,
                }
            )

            logger.info(f"Renamed file: {file_path} -> {new_path}")
            logger.debug("File rename operation completed successfully")
            return 1, str(new_path)
        else:
            if new_path == old_path:
                logger.debug(f"No rename needed: {file_path} already has correct name")
            else:
                logger.debug(f"Target path already exists: {new_path}")
            return 0, file_path

    def _handle_content_violations(
        self, current_file_path: str, file_violations: List[Dict[str, Any]]
    ) -> int:
        """Handle content violations by replacing advanced terms with comprehensive logging."""
        logger.debug(f"Starting content violation handling for {current_file_path}")

        # Handle content violations within files
        content_violations = [v for v in file_violations if v.get("scope") != "file"]

        if not content_violations:
            logger.debug(f"No content violations found for {current_file_path}")
            return 0

        logger.debug(
            "Found %d content violations for %s",
            len(content_violations),
            current_file_path,
        )

        if not os.path.exists(current_file_path):
            logger.error(f"File does not exist: {current_file_path}")
            return 0

        logger.debug(
            f"File exists, proceeding with content processing: {current_file_path}"
        )

        # Create backup if not already done
        backup_path = None
        if current_file_path not in self.files_modified:
            backup_path = self.create_backup(current_file_path)
            logger.debug(f"Backup created for content fixes: {backup_path}")

        # Read and fix content
        logger.debug(f"Reading file content: {current_file_path}")
        with open(current_file_path, "r", encoding="utf-8") as f:
            content = f.read()

        logger.debug(f"File content read, length: {len(content)} characters")

        # Replace 'advanced' with 'sophisticated' in content
        original_content = content
        logger.debug("Applying regex replacements for advanced terms")

        content = re.sub(
            r"\badvanced\b",
            "sophisticated",
            content,
            flags=re.IGNORECASE,
        )
        content = re.sub(r"\bAdvanced\b", "Sophisticated", content)

        if content != original_content:
            logger.debug("Content changes detected, writing updated file")
            with open(current_file_path, "w", encoding="utf-8") as f:
                f.write(content)

            # Track modifications
            was_already_modified = current_file_path in self.files_modified
            if not was_already_modified:
                self.files_modified.add(current_file_path)

            self.fixes_applied.append(
                {
                    "file": current_file_path,
                    "line_number": 0,
                    "original": "advanced/Advanced terms",
                    "fixed": "sophisticated/Sophisticated terms",
                    "fix_type": "replace_advanced_content",
                    "backup_path": backup_path if not was_already_modified else None,
                }
            )

            logger.info(f"Fixed 'advanced' terms in: {current_file_path}")
            logger.debug("Content replacement completed successfully")
            return 0 if was_already_modified else 1
        else:
            logger.debug("No content changes needed")
            return 0

    def fix_advanced_patterns(self) -> int:
        """
        Fix 'advanced' pattern violations by renaming files and updating references.

        Returns:
            Number of files fixed
        """
        # Process and group violations using extracted function
        violations_by_file = self._process_advanced_violations()
        if violations_by_file is None:
            return 0

        files_fixed = 0

        for file_path, file_violations in violations_by_file.items():
            try:
                # Handle filename violations using extracted function
                filename_fixes, current_file_path = self._handle_filename_violations(
                    file_path, file_violations
                )
                files_fixed += filename_fixes

                # Handle content violations using extracted function
                if current_file_path is not None:
                    content_fixes = self._handle_content_violations(
                        current_file_path, file_violations
                    )
                else:
                    content_fixes = 0
                files_fixed += content_fixes

            except Exception as e:
                logger.error(f"Error fixing advanced patterns in {file_path}: {e}")
                continue

        logger.info(f"Fixed 'advanced' patterns in {files_fixed} files")
        return files_fixed

    def fix_magic_numbers(self) -> int:
        """
        Fix magic number violations by adding explanatory comments.

        Note: This is a conservative approach that adds comments rather than
        extracting constants, as that requires more context.

        Returns:
            Number of files fixed
        """
        if not self.violations_data:
            logger.error("No violations data loaded")
            return 0

        # Find and validate magic number violations
        magic_violations = self._find_magic_number_violations()
        if not magic_violations:
            return 0

        # Group violations by file
        violations_by_file = self._group_magic_violations_by_file(magic_violations)

        # Process each file
        files_fixed = self._process_magic_number_files(violations_by_file)

        logger.info(f"Added magic number comments in {files_fixed} files")
        return files_fixed

    def _find_magic_number_violations(self) -> List[Dict[str, Any]]:
        """Find magic number violations in the violations data.

        Returns:
            List of magic number violations
        """
        magic_violations: List[Dict[str, Any]] = []
        for violation in self.violations_data.get("all_violations", []):
            if "magic number" in violation.get("pattern", "").lower():
                magic_violations.append(violation)

        if not magic_violations:
            logger.info("No magic number violations found")
            return []

        logger.info(f"Found {len(magic_violations)} magic number violations")
        return magic_violations

    def _group_magic_violations_by_file(
        self, magic_violations: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Group magic number violations by file.

        Args:
            magic_violations: List of magic number violations

        Returns:
            Dictionary mapping file paths to their violations
        """
        violations_by_file: Dict[str, List[Dict[str, Any]]] = {}
        for violation in magic_violations:
            file_path = violation["file"]
            if file_path not in violations_by_file:
                violations_by_file[file_path] = []
            violations_by_file[file_path].append(violation)

        return violations_by_file

    def _process_magic_number_files(
        self, violations_by_file: Dict[str, List[Dict[str, Any]]]
    ) -> int:
        """Process magic number violations for all files.

        Args:
            violations_by_file: Dictionary mapping files to their violations

        Returns:
            Number of files successfully fixed
        """
        files_fixed = 0

        for file_path, file_violations in violations_by_file.items():
            try:
                if self._fix_magic_numbers_in_file(file_path, file_violations):
                    files_fixed += 1
            except Exception as e:
                logger.error(f"Error fixing magic numbers in {file_path}: {e}")
                continue

        return files_fixed

    def _fix_magic_numbers_in_file(
        self, file_path: str, file_violations: List[Dict[str, Any]]
    ) -> bool:
        """Fix magic numbers in a single file.

        Args:
            file_path: Path to the file to fix
            file_violations: List of violations for this file

        Returns:
            True if file was modified and saved, False otherwise
        """
        # Create backup
        backup_path = self.create_backup(file_path)

        # Read file content
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        # Process violations
        modified = self._add_magic_number_comments(
            lines, file_violations, file_path, backup_path
        )

        if modified:
            with open(file_path, "w", encoding="utf-8") as f:
                f.writelines(lines)

            self.files_modified.add(file_path)
            logger.info(f"Added magic number comments in: {file_path}")
            return True

        return False

    def _add_magic_number_comments(
        self,
        lines: List[str],
        file_violations: List[Dict[str, Any]],
        file_path: str,
        backup_path: str,
    ) -> bool:
        """Add comments to lines with magic numbers.

        Args:
            lines: List of file lines to modify
            file_violations: List of violations for this file
            file_path: Path to the file being processed
            backup_path: Path to the backup file

        Returns:
            True if any lines were modified, False otherwise
        """
        modified = False

        # Process violations (in reverse order to maintain line numbers)
        sorted_violations = sorted(
            file_violations, key=lambda x: x["line_number"], reverse=True
        )

        for violation in sorted_violations:
            line_num = violation["line_number"] - 1  # Convert to 0-based
            if 0 <= line_num < len(lines):
                line = lines[line_num]

                # Add comment if there isn't already one
                if "#" not in line:
                    # Find the magic number in the line
                    numbers = re.findall(r"\b\d{2,}\b", line)
                    if numbers:
                        # Add a comment explaining the number
                        magic_num = numbers[0]
                        lines[line_num] = (
                            line.rstrip()
                            + f"  # NOTE: Magic number {magic_num} should be explained\n"
                        )
                        modified = True

                        self.fixes_applied.append(
                            {
                                "file": file_path,
                                "line_number": violation["line_number"],
                                "original": line.strip(),
                                "fixed": lines[line_num].strip(),
                                "fix_type": "add_magic_number_comment",
                                "backup_path": backup_path,
                            }
                        )

        return modified

    def generate_fixes_report(self, output_path: str) -> Dict[str, Any]:
        """
        Generate report of all fixes applied.

        Args:
            output_path: Path to save the fixes report

        Returns:
            Fixes report dictionary
        """
        report = {
            "fix_timestamp": datetime.now().isoformat(),
            "violations_file": self.violations_file,
            "backup_directory": str(self.backup_dir),
            "total_fixes": len(self.fixes_applied),
            "files_modified": len(self.files_modified),
            "modified_files": list(self.files_modified),
            "fixes_by_type": {},
            "all_fixes": self.fixes_applied,
        }

        # Group fixes by type
        fixes_by_type: Dict[str, List[Dict[str, Any]]] = {}
        for fix in self.fixes_applied:
            fix_type = fix["fix_type"]
            if fix_type not in fixes_by_type:
                fixes_by_type[fix_type] = []
            fixes_by_type[fix_type].append(fix)

        report["fixes_by_type"] = fixes_by_type

        # Save report
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logger.info(f"Fixes report saved to: {output_path}")
        except Exception as e:
            logger.error(f"Error saving fixes report: {e}")

        return report

    def print_fixes_summary(self, report: Dict[str, Any]) -> None:
        """Print summary of fixes applied."""
        print(f"\n{'=' * 60}")
        print("PATTERN FIXES APPLIED REPORT")
        print(f"{'=' * 60}")
        print(f"Fix Date: {report['fix_timestamp']}")
        print(f"Total Fixes: {report['total_fixes']}")
        print(f"Files Modified: {report['files_modified']}")
        print(f"Backup Directory: {report['backup_directory']}")

        if report["fixes_by_type"]:
            print(f"\n{'=' * 60}")
            print("FIXES BY TYPE")
            print(f"{'=' * 60}")

            for fix_type, fixes in report["fixes_by_type"].items():
                print(f"{fix_type.replace('_', ' ').title()}: {len(fixes)} fixes")

        print(f"\n{'=' * 60}")


def main() -> None:
    """Execute main entry point for pattern fixer."""
    parser = argparse.ArgumentParser(
        description="Apply automatic fixes for forbidden pattern violations"
    )
    parser.add_argument(
        "fix_type",
        choices=["print", "advanced", "magic", "all"],
        help="Type of fixes to apply",
    )
    parser.add_argument(
        "--violations-file",
        default="docs/forbidden_patterns_violations.json",
        help="Path to violations JSON report",
    )
    parser.add_argument(
        "--backup-dir", default="backups", help="Directory to store file backups"
    )
    parser.add_argument(
        "--output",
        default="docs/pattern_fixes_report.json",
        help="Output path for fixes report",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be fixed without making changes",
    )
    parser.add_argument(
        "--quiet",
        "-q",
        action="store_true",
        help="Suppress console output except errors",
    )
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    elif args.quiet:
        logging.getLogger().setLevel(logging.ERROR)

    if args.dry_run:
        logger.info("DRY RUN MODE - No changes will be made")
        return

    # Initialize fixer
    fixer = PatternFixer(args.violations_file, args.backup_dir)

    # Load violations
    if not fixer.load_violations():
        sys.exit(1)

    # Apply fixes based on type
    files_fixed = 0

    if args.fix_type == "print":
        files_fixed = fixer.fix_print_statements()
    elif args.fix_type == "advanced":
        files_fixed = fixer.fix_advanced_patterns()
    elif args.fix_type == "magic":
        files_fixed = fixer.fix_magic_numbers()
    elif args.fix_type == "all":
        files_fixed += fixer.fix_print_statements()
        files_fixed += fixer.fix_advanced_patterns()
        files_fixed += fixer.fix_magic_numbers()

    # Generate and save fixes report
    report = fixer.generate_fixes_report(args.output)

    # Print summary unless quiet mode
    if not args.quiet:
        fixer.print_fixes_summary(report)

    logger.info(f"Pattern fixing completed. Modified {files_fixed} files.")
    sys.exit(0)


if __name__ == "__main__":
    main()
