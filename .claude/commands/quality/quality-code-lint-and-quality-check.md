# === Code Lint and Quality Check: Post-Implementation Validation & Quick Fix Protocol ===

**ALWAYS THINK THEN...** Before executing any action, operation, or command in this instruction set, you MUST use thinking to:

1. Analyze the request and understand what needs to be done
2. Plan your approach and identify potential issues
3. Consider the implications and requirements
4. Only then proceed with the actual execution

**This thinking requirement is MANDATORY and must be followed for every action.**

## CANONICAL PROTOCOL ENFORCEMENT - READ FIRST

**THIS SECTION IS MANDATORY AND MUST BE READ, INDEXED, AND FOLLOWED BEFORE ANY COMMAND EXECUTION**

**BEFORE PROCEEDING, YOU MUST ALWAYS:**

1. **READ AND EXECUTE**: `.claude/commands/ai-agent-compliance-prompt.md`
2. **VERIFY**: User has given explicit permission to proceed
3. **ACKNOWLEDGE**: ALL CANONICAL PROTOCOL requirements

**BINDING COMMITMENT**: I hereby commit to strict, unwavering adherence to ALL ai-agent-compliance-prompt.md requirements and will halt operations immediately upon any protocol violation to perform mandatory root cause analysis and corrective action.

**FORBIDDEN**: Proceeding without complete protocol compliance verification

## FORBIDDEN PRACTICES

**FORBIDDEN**
YOU ARE FORBIDDEN from creating custom scripts to fix or remediate the code base, YOU HAVE CONTINUIOUSLY corrupted the code base through poor "fix" scripts
YOU MUST NEVER EVER EVER create custom scripts to fix or remediate the code base

**FORBIDDEN PRACTICES:**

- Making large, non-atomic changes
- Skipping tests or validation
- Ignoring build/deploy errors
- Proceeding without understanding
- Creating duplicate functionality
- Using outdated patterns

**ABSOLUTELY FORBIDDEN - NO EXCEPTIONS:**

- **NO MOCKING** of data or services in production code
- **NO TODOs** - complete ALL work immediately
- **NO SHORTCUTS** - implement properly ALWAYS
- **NO STUBS** - write complete implementations
- **NO FIXED DATA** - use real, dynamic data
- **NO HARDCODED VALUES** - use configuration
- **NO WORKAROUNDS** - fix root causes
- **NO FAKE IMPLEMENTATIONS** - real code only
- **NO PLACEHOLDER CODE** - production-ready only
- **NO TEMPORARY SOLUTIONS** - permanent fixes only

## RTFM (READ THE FUCKING MANUAL) - MANDATORY

**YOU MUST ALWAYS:**

1. **READ JUPYTER NOTEBOOKS:**

   - Search for .ipynb files in the repository
   - Read implementation notebooks for context
   - Review analysis notebooks for insights
   - Study documentation notebooks for patterns

2. **READ PROJECT DOCUMENTATION:**

   - Check `./docs` directory thoroughly
   - Check `./project/docs` if it exists
   - Read ALL README files
   - Review architecture documentation
   - Study API documentation

3. **SEARCH ONLINE FOR BEST PRACTICES:**
   - Use web search for latest documentation
   - Find official framework/library docs
   - Search GitHub for example implementations
   - Review industry best practices
   - Study similar successful projects
   - Check Stack Overflow for common patterns

**SEARCH PRIORITIES:**

- Official documentation (latest version)
- GitHub repositories with high stars
- Industry standard implementations
- Recent blog posts/tutorials (< 1 year old)
- Community best practices

## MCP SERVER TOOLS INTEGRATION

**MANDATORY MCP USAGE:**

### **MCP Memory Management**
**YOU MUST ALWAYS:**
- Add date property to ALL memory and extended memory entries and objects
- Use date property to find, search and query MCP memory and extended-memory entries
- Query past 4-48 hours of session data using current date & time as starting point
- Iteratively search MCP extended-memory to ensure sufficient factual information

### **MCP Extended-Memory**
**YOU MUST ALWAYS:**
- Save ALL context, outcomes, knowledge and decisions to MCP extended-memory
- Update MCP extended-memory when missing or incomplete content exists
- Use MCP extended-memory to understand previous session context before starting
- Search with date props and pagination for past 4-48 hours of entries

### **MCP Memory**
**YOU MUST ALWAYS:**
- Use MCP memory to get current progress, actions and context before proceeding
- Add or update MCP memory entries when content is missing or incomplete
- Ensure current factual information about instruction status, actions and tasks
- Search with date props and pagination for past 4-48 hours of entries

### **MCP Code Research Tools**
**YOU MUST ALWAYS:**
- Use Context7 to research current documentation and best practices
- Use grep to search GitHub for real production code examples and implementations
- Use filesystem to review local codebase for existing patterns and structures
- Use fetch to get additional information and documentation as required
- **FORBIDDEN**: Using filesystem directory_tree (MCP)(path: ".")

### **MCP Planning and Execution Tools**
**YOU MUST ALWAYS:**
- Use thinking to plan tasks and approach
- Use sequential-thinking to breakdown tasks into logical stepwise sequences
- Use claude code sub agents to perform ALL actions and steps
- Use claude code sub agents to complete tasks based on the approach

### **MCP Memory Tracking**
**AT END OF EACH TASK/ACTION/STEP:**
- Use MCP memory to track progress and execution
**AT END OF SESSION:**
- Use MCP extended-memory to persist outcomes, knowledge and decisions

---

## INSTRUCTIONS

### DEVELOPMENT WORKFLOW QUALITY MANDATE - RFC 2119 COMPLIANCE

**THIS IS A DEVELOPMENT WORKFLOW STEP - NOT A COMPREHENSIVE AUDIT:**

- **MUST:** Focus on immediate quality issues in recently changed code
- **MUST:** Run after implementation, refactor, or other coding activities
- **SHALL:** Prepare code for follow-up `code-remediation.md` command if needed
- **MUST:** Fix quick/safe quality issues WITHOUT changing business logic
- **SHALL:** Identify and flag complex issues for remediation command
- **MUST:** Preserve ALL existing functionality and logic during quality improvements

### FUNCTIONALITY PRESERVATION PROTOCOL - CRITICAL

**ABSOLUTE REQUIREMENT - FUNCTIONALITY MUST NEVER BE COMPROMISED:**

**FORBIDDEN CHANGES:**

- **NEVER** alter business logic or algorithms
- **NEVER** change function signatures or return types
- **NEVER** modify API endpoints or data structures
- **NEVER** remove or change feature functionality
- **NEVER** alter configuration values or constants that affect behavior
- **NEVER** change import statements that affect functionality

**PERMITTED CHANGES ONLY:**

- Remove duplicate code blocks (preserve all unique functionality)
- Rename variables/functions for clarity (maintain exact same behavior)
- Reformat code styling and indentation
- Fix linting errors that don't affect logic
- Extract common code into reusable functions (preserve exact behavior)
- Add type annotations without changing logic
- Reduce cyclomatic complexity through structural improvements only

### 4. SINGLE BRANCH DEVELOPMENT STRATEGY - MANDATORY

**FOLLOW THE SINGLE BRANCH DEVELOPMENT PROTOCOL:**

- ALL git workflows MUST follow the protocol defined in `./.claude/commands/core/code-protocol-single-branch-strategy.md`
- **SACRED BRANCHES:** main/master/production are protected - NEVER work directly on them
- **SINGLE WORKING BRANCH:** development branch ONLY - work directly on development
- **NO FEATURE BRANCHES:** FORBIDDEN to create feature/fix branches without explicit permission
- **ATOMIC COMMITS:** One logical change per commit with conventional format
- **IMMEDIATE BACKUP:** Push to origin after every commit

**COMMIT MESSAGE FORMAT FOR QUALITY FIXES:**

```
refactor(quality): improve code quality in [area]

- Fixed linting issues in [specific files]
- Reduced complexity in [specific functions]
- Removed duplicate code blocks in [specific areas]
- Applied formatting standards to [specific files]
- NO FUNCTIONALITY CHANGES - quality improvements only

[AI-Instance-ID-Timestamp]
```

## CODE LINT AND QUALITY CHECK PROTOCOL

Execute post-development lint and quality check with immediate fixes for: **$argument**

**WORKFLOW CONTEXT:**
This command runs as a development step after coding activities to catch immediate linting and quality issues. Complex issues will be flagged for the follow-up `code-remediation.md` command.

### PHASE 1: SCOPE DEFINITION AND VALIDATION

**STEP 1: TARGET AREA IDENTIFICATION**

```bash
# Define the target area for quality analysis
TARGET_AREA="$argument"

# Validate target area exists
if [ ! -d "$TARGET_AREA" ] && [ ! -f "$TARGET_AREA" ]; then
    echo "ERROR: Target area '$TARGET_AREA' does not exist"
    echo "Please specify a valid directory or file path"
    exit 1
fi

# Log analysis scope
echo "=== DEVELOPMENT QUALITY CHECK ===" > .code_quality_report.log
echo "Target Area: $TARGET_AREA" >> .code_quality_report.log
echo "Workflow Step: Post-implementation quality check" >> .code_quality_report.log
echo "Analysis Started: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> .code_quality_report.log
echo "AI Instance: [AI-Instance-ID-$(date -u +%Y-%m-%dT%H:%M:%SZ)]" >> .code_quality_report.log
echo "Follow-up: Issues requiring complex fixes will be flagged for code-remediation.md" >> .code_quality_report.log
echo "" >> .code_quality_report.log
```

**STEP 2: PRODUCTION CODE IDENTIFICATION**

```bash
# Identify production code files (exclude tests, demos, docs)
echo "=== PRODUCTION CODE FILES IDENTIFIED ===" >> .code_quality_report.log

# Find Python production files
find "$TARGET_AREA" -name "*.py" \
    -not -path "*/test*" \
    -not -path "*/demo*" \
    -not -path "*/example*" \
    -not -path "*/__pycache__*" \
    -not -name "test_*.py" \
    -not -name "*_test.py" \
    -not -name "demo_*.py" \
    > .production_python_files.txt

# Find JavaScript/TypeScript production files
find "$TARGET_AREA" -name "*.js" -o -name "*.ts" -o -name "*.jsx" -o -name "*.tsx" \
    -not -path "*/test*" \
    -not -path "*/demo*" \
    -not -path "*/example*" \
    -not -path "*/node_modules*" \
    -not -name "*.test.*" \
    -not -name "*.spec.*" \
    > .production_js_files.txt

# Log file counts
echo "Python Production Files: $(wc -l < .production_python_files.txt)" >> .code_quality_report.log
echo "JavaScript/TypeScript Production Files: $(wc -l < .production_js_files.txt)" >> .code_quality_report.log
echo "" >> .code_quality_report.log
```

### PHASE 2: RAPID QUALITY ANALYSIS (DEVELOPMENT FOCUSED)

**STEP 1: LINTING ANALYSIS**

```bash
echo "=== LINTING ANALYSIS ===" >> .code_quality_report.log

# Python linting
if [ -s .production_python_files.txt ]; then
    echo "Running Python linting analysis..." >> .code_quality_report.log

    # Flake8 analysis
    if command -v flake8 &> /dev/null; then
        flake8 --config=setup.cfg --output-file=.flake8_results.txt $(cat .production_python_files.txt) || true
        echo "Flake8 Issues: $(wc -l < .flake8_results.txt)" >> .code_quality_report.log
    fi

    # Pylint analysis
    if command -v pylint &> /dev/null; then
        pylint --output-format=text --output=.pylint_results.txt $(cat .production_python_files.txt) || true
        echo "Pylint Analysis Completed" >> .code_quality_report.log
    fi
fi

# JavaScript/TypeScript linting
if [ -s .production_js_files.txt ]; then
    echo "Running JavaScript/TypeScript linting analysis..." >> .code_quality_report.log

    if command -v eslint &> /dev/null; then
        eslint --format=unix --output-file=.eslint_results.txt $(cat .production_js_files.txt) || true
        echo "ESLint Issues: $(wc -l < .eslint_results.txt)" >> .code_quality_report.log
    fi
fi

echo "" >> .code_quality_report.log
```

**STEP 2: TYPE CHECKING ANALYSIS**

```bash
echo "=== TYPE CHECKING ANALYSIS ===" >> .code_quality_report.log

# Python type checking
if [ -s .production_python_files.txt ]; then
    if command -v mypy &> /dev/null; then
        mypy --config-file=mypy.ini --output=.mypy_results.txt $(cat .production_python_files.txt) || true
        echo "MyPy Type Issues: $(wc -l < .mypy_results.txt)" >> .code_quality_report.log
    fi
fi

# TypeScript type checking
if [ -s .production_js_files.txt ]; then
    if command -v tsc &> /dev/null; then
        tsc --noEmit --listFiles > .tsc_results.txt 2>&1 || true
        echo "TypeScript Analysis Completed" >> .code_quality_report.log
    fi
fi

echo "" >> .code_quality_report.log
```

**STEP 3: COMPLEXITY ANALYSIS**

```bash
echo "=== COMPLEXITY ANALYSIS ===" >> .code_quality_report.log

# Python complexity analysis
if [ -s .production_python_files.txt ]; then
    if command -v radon &> /dev/null; then
        radon cc --min=C --output-file=.complexity_results.txt $(cat .production_python_files.txt) || true
        echo "High Complexity Functions: $(wc -l < .complexity_results.txt)" >> .code_quality_report.log
    fi
fi

# JavaScript complexity analysis
if [ -s .production_js_files.txt ]; then
    if command -v complexity-report &> /dev/null; then
        complexity-report --output=json --output-file=.js_complexity_results.json $(cat .production_js_files.txt) || true
        echo "JavaScript Complexity Analysis Completed" >> .code_quality_report.log
    fi
fi

echo "" >> .code_quality_report.log
```

**STEP 4: DEDUPLICATION ANALYSIS**

```bash
echo "=== DEDUPLICATION ANALYSIS ===" >> .code_quality_report.log

# Python duplicate code detection
if [ -s .production_python_files.txt ]; then
    if command -v pylint &> /dev/null; then
        pylint --disable=all --enable=duplicate-code --output=.duplicate_code_results.txt $(cat .production_python_files.txt) || true
        echo "Python Duplicate Code Blocks: $(grep -c "Similar lines" .duplicate_code_results.txt || echo 0)" >> .code_quality_report.log
    fi
fi

# Cross-language duplicate detection using custom analysis
python3 << 'EOF'
import os
import hashlib
from collections import defaultdict

def analyze_duplicates(files_list):
    code_blocks = defaultdict(list)

    with open(files_list, 'r') as f:
        files = f.read().strip().split('\n')

    for file_path in files:
        if not file_path or not os.path.exists(file_path):
            continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # Analyze blocks of 5+ lines
            for i in range(len(lines) - 4):
                block = ''.join(lines[i:i+5]).strip()
                if len(block) > 100:  # Minimum meaningful block size
                    block_hash = hashlib.md5(block.encode()).hexdigest()
                    code_blocks[block_hash].append((file_path, i+1))
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")

    # Report duplicates
    duplicates_found = 0
    with open('.deduplication_analysis.txt', 'w') as f:
        for block_hash, locations in code_blocks.items():
            if len(locations) > 1:
                duplicates_found += 1
                f.write(f"Duplicate block found in {len(locations)} locations:\n")
                for file_path, line_num in locations:
                    f.write(f"  {file_path} (line {line_num})\n")
                f.write("\n")

    print(f"Total duplicate blocks found: {duplicates_found}")

# Analyze Python files
if os.path.exists('.production_python_files.txt'):
    analyze_duplicates('.production_python_files.txt')

# Analyze JavaScript files
if os.path.exists('.production_js_files.txt'):
    analyze_duplicates('.production_js_files.txt')
EOF

echo "Custom Deduplication Analysis Completed" >> .code_quality_report.log
echo "" >> .code_quality_report.log
```

### PHASE 3: IMMEDIATE SAFE FIXES (COMPLEX ISSUES FLAGGED FOR REMEDIATION)

**STEP 1: LINTING FIXES (FUNCTIONALITY PRESERVING ONLY)**

```bash
echo "=== APPLYING LINTING FIXES ===" >> .code_quality_report.log

# Python linting fixes
if [ -s .production_python_files.txt ]; then
    echo "Applying Python linting fixes..." >> .code_quality_report.log

    # Use autopep8 for safe formatting fixes
    if command -v autopep8 &> /dev/null; then
        while IFS= read -r file; do
            if [ -f "$file" ]; then
                autopep8 --in-place --aggressive --aggressive "$file"
                echo "Fixed formatting: $file" >> .code_quality_report.log
            fi
        done < .production_python_files.txt
    fi

    # Use isort for import organization
    if command -v isort &> /dev/null; then
        while IFS= read -r file; do
            if [ -f "$file" ]; then
                isort "$file"
                echo "Fixed imports: $file" >> .code_quality_report.log
            fi
        done < .production_python_files.txt
    fi
fi

# JavaScript/TypeScript formatting fixes
if [ -s .production_js_files.txt ]; then
    echo "Applying JavaScript/TypeScript formatting fixes..." >> .code_quality_report.log

    if command -v prettier &> /dev/null; then
        while IFS= read -r file; do
            if [ -f "$file" ]; then
                prettier --write "$file"
                echo "Fixed formatting: $file" >> .code_quality_report.log
            fi
        done < .production_js_files.txt
    fi
fi

# Commit formatting fixes
git add .
git commit -m "style(dev-quality): post-implementation formatting fixes

- Applied formatting standards after recent development work
- Fixed import organization and code style
- Development workflow step - preparing for potential remediation
- NO FUNCTIONALITY CHANGES - style fixes only

[AI-Instance-ID-$(date -u +%Y-%m-%dT%H:%M:%SZ)]"

git push origin development
echo "Development quality fixes committed and pushed" >> .code_quality_report.log
```

**STEP 2: COMPLEXITY IDENTIFICATION (FLAG FOR REMEDIATION)**

```bash
echo "=== FLAGGING COMPLEXITY ISSUES FOR REMEDIATION ===" >> .code_quality_report.log

# Manual complexity reduction for high-complexity functions
# This requires careful analysis to preserve functionality
python3 << 'EOF'
import ast
import os
import re

def analyze_complexity_and_suggest_fixes(files_list):
    if not os.path.exists(files_list):
        return

    with open(files_list, 'r') as f:
        files = f.read().strip().split('\n')

    suggestions = []

    for file_path in files:
        if not file_path or not os.path.exists(file_path):
            continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content, filename=file_path)

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # Count decision points (if, for, while, try, except, with)
                    complexity = 1  # Base complexity
                    for child in ast.walk(node):
                        if isinstance(child, (ast.If, ast.For, ast.While, ast.Try, ast.With)):
                            complexity += 1
                        elif isinstance(child, ast.ExceptHandler):
                            complexity += 1

                    if complexity > 10:  # High complexity threshold
                        suggestions.append({
                            'file': file_path,
                            'function': node.name,
                            'line': node.lineno,
                            'complexity': complexity,
                            'suggestion': 'Consider extracting helper functions to reduce complexity'
                        })

        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")

    # Write suggestions to file
    with open('.complexity_suggestions.txt', 'w') as f:
        for suggestion in suggestions:
            f.write(f"HIGH COMPLEXITY: {suggestion['file']}:{suggestion['line']}\n")
            f.write(f"Function: {suggestion['function']} (complexity: {suggestion['complexity']})\n")
            f.write(f"Suggestion: {suggestion['suggestion']}\n\n")

    print(f"Complexity analysis complete. {len(suggestions)} high-complexity functions identified.")

# Analyze Python files
if os.path.exists('.production_python_files.txt'):
    analyze_complexity_and_suggest_fixes('.production_python_files.txt')
EOF

echo "Complexity issues flagged for code-remediation.md command" >> .code_quality_report.log
```

**STEP 3: DEDUPLICATION FIXES (PRESERVE ALL FUNCTIONALITY)**

```bash
echo "=== APPLYING DEDUPLICATION FIXES ===" >> .code_quality_report.log

# Apply safe deduplication fixes
python3 << 'EOF'
import os
import ast
import difflib
from collections import defaultdict

def safe_deduplication_fixes(files_list):
    if not os.path.exists(files_list):
        return

    with open(files_list, 'r') as f:
        files = f.read().strip().split('\n')

    # Only apply SAFE deduplication fixes that preserve functionality
    duplicates_fixed = 0

    # For each file, look for obvious duplicates (imports, constants, etc.)
    for file_path in files:
        if not file_path or not os.path.exists(file_path):
            continue

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # Remove duplicate import statements (safe)
            seen_imports = set()
            cleaned_lines = []

            for line in lines:
                stripped = line.strip()

                # Handle duplicate imports
                if stripped.startswith(('import ', 'from ')) and stripped.endswith(('import *', 'import')):
                    if stripped not in seen_imports:
                        seen_imports.add(stripped)
                        cleaned_lines.append(line)
                    else:
                        duplicates_fixed += 1
                        print(f"Removed duplicate import in {file_path}: {stripped}")
                else:
                    cleaned_lines.append(line)

            # Only write back if changes were made
            if len(cleaned_lines) != len(lines):
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.writelines(cleaned_lines)
                print(f"Fixed {len(lines) - len(cleaned_lines)} duplicates in {file_path}")

        except Exception as e:
            print(f"Error processing {file_path}: {e}")

    print(f"Safe deduplication complete. {duplicates_fixed} duplicates removed.")

# Apply to Python files
if os.path.exists('.production_python_files.txt'):
    safe_deduplication_fixes('.production_python_files.txt')

# Apply to JavaScript files
if os.path.exists('.production_js_files.txt'):
    safe_deduplication_fixes('.production_js_files.txt')
EOF

# Commit deduplication fixes if any were made
if [ -n "$(git status --porcelain)" ]; then
    git add .
    git commit -m "refactor(dev-quality): remove safe duplicate patterns

- Removed duplicate import statements after recent development
- Development workflow step - safe deduplication only
- Complex duplicates flagged for code-remediation.md
- NO BEHAVIOR CHANGES - safe cleanup only

[AI-Instance-ID-$(date -u +%Y-%m-%dT%H:%M:%SZ)]"

    git push origin development
    echo "Safe deduplication fixes committed and pushed" >> .code_quality_report.log
else
    echo "No safe deduplication fixes needed" >> .code_quality_report.log
fi
```

### PHASE 4: VALIDATION AND VERIFICATION

**STEP 1: FUNCTIONALITY VERIFICATION**

```bash
echo "=== FUNCTIONALITY VERIFICATION ===" >> .code_quality_report.log

# Build verification
if [ -f "Makefile" ]; then
    make build >> .code_quality_report.log 2>&1
    if [ $? -eq 0 ]; then
        echo "✅ Build successful after quality fixes" >> .code_quality_report.log
    else
        echo "❌ Build failed - quality fixes may have introduced issues" >> .code_quality_report.log
    fi
elif [ -f "package.json" ]; then
    npm run build >> .code_quality_report.log 2>&1
    if [ $? -eq 0 ]; then
        echo "✅ Build successful after quality fixes" >> .code_quality_report.log
    else
        echo "❌ Build failed - quality fixes may have introduced issues" >> .code_quality_report.log
    fi
fi

# Test verification (if tests exist)
if [ -f "Makefile" ]; then
    make test >> .code_quality_report.log 2>&1
    if [ $? -eq 0 ]; then
        echo "✅ All tests pass after quality fixes" >> .code_quality_report.log
    else
        echo "⚠️ Some tests failed - review required" >> .code_quality_report.log
    fi
elif [ -f "package.json" ]; then
    npm test >> .code_quality_report.log 2>&1
    if [ $? -eq 0 ]; then
        echo "✅ All tests pass after quality fixes" >> .code_quality_report.log
    else
        echo "⚠️ Some tests failed - review required" >> .code_quality_report.log
    fi
fi
```

**STEP 2: FINAL QUALITY REPORT**

```bash
echo "=== DEVELOPMENT QUALITY CHECK REPORT ===" >> .code_quality_report.log
echo "Analysis Completed: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> .code_quality_report.log
echo "" >> .code_quality_report.log

echo "IMMEDIATE FIXES APPLIED:" >> .code_quality_report.log
echo "- Code formatting and style fixes applied" >> .code_quality_report.log
echo "- Import organization completed" >> .code_quality_report.log
echo "- Safe duplicate imports/patterns removed" >> .code_quality_report.log
echo "- Basic linting issues resolved" >> .code_quality_report.log
echo "" >> .code_quality_report.log

echo "ISSUES FLAGGED FOR REMEDIATION COMMAND:" >> .code_quality_report.log
echo "- High complexity functions identified" >> .code_quality_report.log
echo "- Complex duplicate code patterns detected" >> .code_quality_report.log
echo "- Type checking issues requiring attention" >> .code_quality_report.log
echo "- See .complexity_suggestions.txt for details" >> .code_quality_report.log
echo "" >> .code_quality_report.log

echo "FUNCTIONALITY PRESERVATION:" >> .code_quality_report.log
echo "- NO business logic changes made" >> .code_quality_report.log
echo "- NO API changes made" >> .code_quality_report.log
echo "- NO configuration changes made" >> .code_quality_report.log
echo "- ALL features preserved" >> .code_quality_report.log
echo "" >> .code_quality_report.log

echo "NEXT STEPS:" >> .code_quality_report.log
echo "- Run code-remediation.md command to address complex issues" >> .code_quality_report.log
echo "- Review .complexity_suggestions.txt for refactoring opportunities" >> .code_quality_report.log
echo "- Consider addressing flagged type checking issues" >> .code_quality_report.log
echo "" >> .code_quality_report.log

# Display final report
cat .code_quality_report.log

# Cleanup temporary files
rm -f .production_python_files.txt .production_js_files.txt
rm -f .flake8_results.txt .pylint_results.txt .eslint_results.txt
rm -f .mypy_results.txt .tsc_results.txt .complexity_results.txt
rm -f .js_complexity_results.json .duplicate_code_results.txt
rm -f .deduplication_analysis.txt .complexity_suggestions.txt
```

## QUALITY ASSURANCE CHECKLIST

**MANDATORY VERIFICATION BEFORE COMPLETION:**

- [ ] All linting issues resolved without functionality changes
- [ ] Type checking issues addressed appropriately
- [ ] Code formatting applied consistently
- [ ] Safe duplicate code removed (imports, obvious duplicates only)
- [ ] High complexity functions identified for future refactoring
- [ ] Build process still works after changes
- [ ] Tests still pass (if they exist)
- [ ] No business logic alterations made
- [ ] No API or interface changes made
- [ ] All commits follow conventional format
- [ ] Changes pushed to development branch

**CRITICAL SUCCESS CRITERIA:**

✅ **FUNCTIONALITY PRESERVED:** All existing features work exactly as before
✅ **QUALITY IMPROVED:** Code is cleaner, more maintainable, and follows standards
✅ **BUILD STABLE:** Application builds and runs without errors
✅ **TESTS PASSING:** All existing tests continue to pass
✅ **NO REGRESSIONS:** No new bugs or issues introduced

## POST-COMPLETION ACTIONS

**MANDATORY CLEANUP AND DOCUMENTATION:**

```bash
# Final commit with comprehensive summary and date stamp
CURRENT_TIMESTAMP=$(date -u +%Y-%m-%d-%H%M%S)
git add .
git commit -m "refactor(quality): comprehensive code quality improvements

- Applied formatting standards across $(echo $TARGET_AREA)
- Fixed linting issues without functionality changes
- Removed safe duplicate code patterns
- Organized imports and code structure
- Identified high-complexity areas for future refactoring
- NO FUNCTIONALITY CHANGES - quality improvements only
- Build verified, tests passing, no regressions
- Generated deliverables with timestamp: ${CURRENT_TIMESTAMP}

🤖 Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>"

git push origin development

# Generate final deliverables with date stamps
echo "=== FINAL DELIVERABLES WITH DATE STAMPS ===" >> .code_quality_report.log
echo "Date Stamp Format: ${CURRENT_TIMESTAMP}" >> .code_quality_report.log
echo "" >> .code_quality_report.log
echo "Required Outputs Generated:" >> .code_quality_report.log
echo "- Code_Lint_Quality_Check_Report-${CURRENT_TIMESTAMP}.txt" >> .code_quality_report.log
echo "- Complexity_Analysis_Results-${CURRENT_TIMESTAMP}.txt" >> .code_quality_report.log
echo "- Deduplication_Findings-${CURRENT_TIMESTAMP}.txt" >> .code_quality_report.log
echo "- Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt" >> .code_quality_report.log
echo "" >> .code_quality_report.log

# Copy final report with timestamp
cp .code_quality_report.log "Code_Lint_Quality_Check_Report-${CURRENT_TIMESTAMP}.txt"
if [ -f .complexity_suggestions.txt ]; then
    cp .complexity_suggestions.txt "Complexity_Analysis_Results-${CURRENT_TIMESTAMP}.txt"
fi
if [ -f .deduplication_analysis.txt ]; then
    cp .deduplication_analysis.txt "Deduplication_Findings-${CURRENT_TIMESTAMP}.txt"
fi

# Create summary with date stamp
echo "Code Lint and Quality Check Summary - ${CURRENT_TIMESTAMP}" > "Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt"
echo "==========================================================" >> "Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt"
echo "" >> "Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt"
echo "Analysis completed with comprehensive quality improvements" >> "Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt"
echo "All deliverables generated with reverse date stamp format" >> "Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt"
echo "Date stamp used: ${CURRENT_TIMESTAMP}" >> "Quality_Improvement_Summary-${CURRENT_TIMESTAMP}.txt"

echo "✅ Development quality check completed for: $TARGET_AREA"
echo "✅ Immediate fixes applied and committed to development branch"
echo "✅ Functionality preserved, ready for potential remediation phase"
echo ""
echo "📋 SUMMARY:"
echo "   - Style and formatting: FIXED"
echo "   - Safe duplicates: CLEANED"
echo "   - Complex issues: FLAGGED for remediation"
echo "   - Build status: VERIFIED"
echo ""
echo "🔄 NEXT: Run code-remediation.md if complex issues need addressing"
```

---

**ENFORCEMENT:** This protocol ensures code quality improvements while maintaining absolute functionality preservation. NO compromises on existing features or business logic are permitted.
