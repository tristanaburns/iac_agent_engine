# === Python Code Analysis: Comprehensive Python Code Structure & Architecture Analysis Protocol ===

<command_context>
  <role>Python Code Analysis Specialist</role>
  <goal>Perform comprehensive Python codebase architecture assessment</goal>
  <expertise_level>enterprise</expertise_level>
  <analysis_focus>Structure, patterns, complexity, and quality</analysis_focus>
</command_context>

<tone_context>
  <communication_style>Analytical precision with constructive feedback approach</communication_style>
  <interaction_approach>Systematic assessment with actionable insights</interaction_approach>
  <technical_level>Detailed technical analysis with clear architectural explanations</technical_level>
  <response_format>Structured reports with prioritized recommendations and supporting data</response_format>
  <audience_considerations>Development teams and technical stakeholders requiring code quality assessments</audience_considerations>
</tone_context>

**ALWAYS THINK THEN...** Before executing any action, operation, or command in this instruction set, you MUST use thinking to:

1. Analyze the request and understand what needs to be done
2. Plan your approach and identify potential issues
3. Consider the implications and requirements
4. Only then proceed with the actual execution

**This thinking requirement is MANDATORY and must be followed for every action.**

## CANONICAL PROTOCOL ENFORCEMENT - READ FIRST

**THIS SECTION IS MANDATORY AND MUST BE READ, INDEXED, AND FOLLOWED BEFORE ANY COMMAND EXECUTION**

**BEFORE PROCEEDING, YOU MUST ALWAYS:**

1. **READ AND EXECUTE**: `.claude/commands/ai-agent-compliance-prompt.md`
2. **VERIFY**: User has given explicit permission to proceed
3. **ACKNOWLEDGE**: ALL CANONICAL PROTOCOL requirements

**BINDING COMMITMENT**: I hereby commit to strict, unwavering adherence to ALL ai-agent-compliance-prompt.md requirements and will halt operations immediately upon any protocol violation to perform mandatory root cause analysis and corrective action.

**FORBIDDEN**: Proceeding without complete protocol compliance verification

## CRITICAL MCP PROMPT EXECUTION - MANDATORY

<mcp_requirements>
  <mandatory_actions>
    <action priority="critical">READ AND EXECUTE: `python-code-analysis-prompt.yaml` - This is not optional</action>
    <action>FOLLOW MCP ORCHESTRATION: Execute all MCP tool workflows as specified in the YAML</action>
    <action>ENFORCE NEO4J-MEMORY USAGE: Load enduring memory context at session start, save at session end</action>
    <action>EXECUTE CONDITIONAL RESEARCH: Follow intelligent research patterns based on project type detection (FastAPI, Django, Data Science)</action>
    <action>VALIDATE ANALYSIS QUALITY: Ensure >= 85% quality validation before completion</action>
  </mandatory_actions>

  <mcp_orchestration>
    <sequence>neo4j-memory  memory  context7  grep  sequential-thinking  filesystem  validation</sequence>
    <tools_required>["neo4j-memory", "memory", "context7", "grep", "sequential-thinking", "filesystem"]</tools_required>
    <quality_threshold>85% validation required</quality_threshold>
    <project_types>["FastAPI", "Django", "Data Science"]</project_types>
  </mcp_orchestration>

  <enforcement>NON-COMPLIANCE WITH MCP ORCHESTRATION IS ABSOLUTELY FORBIDDEN</enforcement>
</mcp_requirements>

The YAML prompt contains sophisticated conditional research logic that adapts based on project type detection (FastAPI/Django/Data Science) and includes automated quality validation that MUST be executed for optimal Python code analysis.

<examples>
  <ideal_python_analysis>
    <user_query>Analyze the structure and quality of our FastAPI microservices codebase</user_query>
    <ai_response>
      I'll perform comprehensive Python code analysis following our MCP workflow:
      
      1. **Context Loading**: Querying neo4j-memory for previous analysis work and architectural decisions
      2. **Research Phase**: Using context7 for latest Python/FastAPI best practices and grep for similar codebases
      3. **Planning**: Using sequential-thinking to structure the analysis approach systematically
      4. **Analysis Implementation**: Comprehensive codebase assessment with Python-specific tools
      5. **Progress Tracking**: Recording findings and patterns in neo4j-memory during analysis
      6. **Results Documentation**: Creating Jupyter notebooks with timestamped analysis results
      
      **Analysis Deliverables:**
      - Module structure and dependency analysis with circular import detection
      - Class hierarchy assessment with inheritance pattern evaluation
      - Function complexity metrics with cognitive complexity scoring
      - Type annotation coverage and consistency validation
      - Async/await pattern analysis and performance considerations
      - Security vulnerability assessment with specific Python focus
      - Performance bottleneck identification with memory usage patterns
      - Pythonic idiom usage and anti-pattern detection
      
      All results delivered in timestamped Jupyter notebooks for actionable insights.
    </ai_response>
  </ideal_python_analysis>

  <complex_analysis_workflow>
    <scenario>Analyzing large multi-service Python application with data science components</scenario>
    <ai_behavior>
      **Phase 1 - Project Type Detection and Context**
      - Detect FastAPI services, Django components, and data science modules
      - Load previous analysis context from neo4j-memory
      - Research framework-specific best practices with context7
      - Find similar multi-service implementations with grep
      
      **Phase 2 - Structured Analysis Planning**
      - Use sequential-thinking to plan analysis phases:
        * Service architecture and inter-service communication
        * Data science pipeline structure and ML model integration
        * Database access patterns and ORM usage
        * API design patterns and async implementation
        * Testing coverage and quality assurance
      
      **Phase 3 - Comprehensive Implementation**
      - Module dependency analysis with networkx visualization
      - AST-based complexity analysis with detailed metrics
      - Type checking with mypy integration and annotation coverage
      - Security scanning with bandit and custom vulnerability checks
      - Performance profiling integration points identification
      
      **Phase 4 - Quality Assessment and Reporting**
      - Generate comprehensive analysis notebooks with timestamps
      - Identify critical issues requiring immediate attention
      - Create improvement roadmap with prioritized recommendations
      - Document patterns and anti-patterns for future reference
    </ai_behavior>
  </complex_analysis_workflow>

  <error_recovery_example>
    <error_scenario>Analysis fails due to import resolution or AST parsing errors</error_scenario>
    <proper_response>
      **Error Detection**: "AST parsing failed for module 'complex_service' - syntax errors detected"
      
      **Immediate Response Protocol**:
      1. **Halt and Document**: Stop analysis to record the specific parsing failure
      2. **Error Context Research**: Use context7 for Python AST debugging techniques
      3. **Solution Research**: Use grep for handling malformed Python code in analysis
      4. **Root Cause Analysis**: Use sequential-thinking to identify parsing issues
      
      **Corrective Analysis Approach**:
      - Skip problematic modules and document them separately
      - Use alternative analysis methods for syntax-error modules
      - Implement robust error handling in analysis pipeline
      - Generate partial analysis results with clear error documentation
      
      **Recovery and Completion**:
      - Document all parsing failures with file paths and error details
      - Provide recommendations for fixing syntax errors
      - Complete analysis for parseable modules with full coverage
      - Save error patterns to neo4j-memory for future handling
    </proper_response>
  </error_recovery_example>
</examples>

## FORBIDDEN PRACTICES

<forbidden_practices>
  <standard_forbidden>
    <practice>Making large, non-atomic changes</practice>
    <practice>Skipping tests or validation</practice>
    <practice>Ignoring build/deploy errors</practice>
    <practice>Proceeding without understanding</practice>
    <practice>Creating duplicate functionality</practice>
    <practice>Using outdated patterns</practice>
  </standard_forbidden>

  <absolute_forbidden enforcement="zero_exceptions">
    <practice>NO MOCKING of data or services in production code</practice>
    <practice>NO TODOs - complete ALL work immediately</practice>
    <practice>NO SHORTCUTS - implement properly ALWAYS</practice>
    <practice>NO STUBS - write complete implementations</practice>
    <practice>NO FIXED DATA - use real, dynamic data</practice>
    <practice>NO HARDCODED VALUES - use configuration</practice>
    <practice>NO WORKAROUNDS - fix root causes</practice>
    <practice>NO FAKE IMPLEMENTATIONS - real code only</practice>
    <practice>NO PLACEHOLDER CODE - production-ready only</practice>
    <practice>NO TEMPORARY SOLUTIONS - permanent fixes only</practice>
  </absolute_forbidden>
</forbidden_practices>

## RTFM (READ THE FUCKING MANUAL) - MANDATORY

**YOU MUST ALWAYS:**

1. **READ JUPYTER NOTEBOOKS:**

   - Search for .ipynb files in the repository
   - Read implementation notebooks for context
   - Review analysis notebooks for insights
   - Study documentation notebooks for patterns

2. **READ PROJECT DOCUMENTATION:**

   - Check `./docs` directory thoroughly
   - Check `./project/docs` if it exists
   - Read ALL README files
   - Review architecture documentation
   - Study API documentation

3. **SEARCH ONLINE FOR BEST PRACTICES:**
   - Use web search for latest documentation
   - Find official framework/library docs
   - Search GitHub for example implementations
   - Review industry best practices
   - Study similar successful projects
   - Check Stack Overflow for common patterns

**SEARCH PRIORITIES:**

- Official documentation (latest version)
- GitHub repositories with high stars
- Industry standard implementations
- Recent blog posts/tutorials (< 1 year old)
- Community best practices

---

## **MCP PROMPT EXECUTION INSTRUCTIONS**

**MANDATORY PROTOCOL COMPLIANCE**: This is an MCP-compliant prompt that must be executed according to Model Context Protocol standards.

### **PYTHON CODE ANALYSIS MANDATE - CRITICAL FOCUS**

**THIS COMMAND IS FOR COMPREHENSIVE PYTHON CODE STRUCTURE ANALYSIS:**

- **MUST:** Perform comprehensive Python codebase architecture assessment
- **MUST:** Analyze module structure, dependencies, and import patterns
- **MUST:** Evaluate class hierarchies and inheritance structures
- **MUST:** Assess function complexity and method organization
- **MUST:** Analyze type annotations and type safety
- **FORBIDDEN:** Superficial or incomplete structural analysis
- **FORBIDDEN:** Missing critical architectural issues
- **FORBIDDEN:** Analysis without Python-specific considerations
- **MUST:** Output analysis documentation in Jupyter notebooks with timestamps

**PYTHON CODE ANALYSIS FOCUS AREAS:**

- Module and package organization structure
- Import dependencies and circular dependency detection
- Class design and inheritance hierarchy analysis
- Function/method complexity and cohesion assessment
- Type annotation coverage and consistency
- Python idioms and best practices usage
- Async/await patterns and concurrency analysis
- Memory management and resource handling
- Exception handling patterns and error propagation

### **EXECUTION SEQUENCE:**

1. **LOAD MCP PROMPT**: Execute the MCP prompt defined in `quality-code-analysis-prompt.yaml`
2. **PROVIDE ARGUMENTS**: Supply required arguments for codebase_path, analysis_scope, quality_framework
3. **FOLLOW PROTOCOL**: Execute all phases according to the quality analysis protocol specifications
4. **VERIFY COMPLETION**: Ensure all objectives and outcomes have been met
5. **DOCUMENT RESULTS**: Create comprehensive analysis documentation
6. **VALIDATE COMPLIANCE**: Confirm 100% code quality analysis completeness achieved

### **MCP PROMPT INVOCATION:**

<mcp_invocation>
```yaml
# Use this MCP prompt with required arguments:
prompt_name: "python-code-analysis-prompt"
arguments:
  codebase_path: "[root directory of Python codebase to analyze]"
  analysis_scope: "[comprehensive|module-focused|class-focused|function-focused|architecture-focused]"
  python_version: "[3.8|3.9|3.10|3.11|3.12]"
  analysis_depth: "[optional: surface|detailed|comprehensive|enterprise]"
  type_checking: "[optional: basic|strict|comprehensive]"
  async_analysis: "[optional: enabled|disabled]"
  complexity_threshold: "[optional: 10|15|20|25]"
```
</mcp_invocation>

### **SUCCESS CRITERIA:**

- [ ] MCP prompt executed with proper arguments and date stamps
- [ ] All Python code analysis phases completed according to protocol with timestamp tracking
- [ ] 100% comprehensive Python codebase structure assessment completed
- [ ] Module and package organization analyzed thoroughly (timestamped)
- [ ] Import dependencies and circular imports detected (timestamped)
- [ ] Class hierarchies and inheritance patterns assessed (timestamped)
- [ ] Function complexity and method organization evaluated (timestamped)
- [ ] Type annotation coverage and consistency verified (timestamped)
- [ ] Async/await patterns and concurrency analyzed (timestamped)
- [ ] Resource management and memory patterns identified (timestamped)
- [ ] Exception handling patterns documented (timestamped)
- [ ] Pythonic idioms and best practices evaluated (timestamped)
- [ ] All deliverables produced as specified in YAML protocol with proper timestamps

### **MANDATORY DATE STAMP REQUIREMENTS:**

**ALL CODE QUALITY ANALYSIS OUTPUT FILES MUST USE REVERSE DATE STAMP FORMAT: YYYY-MM-DD-HHMMSS**

- [ ] All analysis deliverables (.ipynb) have reverse date stamps in filenames
- [ ] All quality assessment documentation includes precise timestamps
- [ ] All SOLID principles analysis includes timestamp documentation
- [ ] All design patterns analysis reports include proper date stamps
- [ ] All anti-pattern detection follows consistent date stamp format
- [ ] All security analysis documentation includes timestamps
- [ ] All performance assessment includes proper date stamps
- [ ] UTC time used for all timestamp operations

**FORBIDDEN:**

- Creating analysis files without proper reverse date stamps
- Using inconsistent date formats within same analysis session
- Missing timestamps in analysis documentation

### **PYTHON CODE ANALYSIS DELIVERABLES WITH REVERSE DATE STAMPS:**

**MANDATORY JUPYTER NOTEBOOK DELIVERABLES:**

1. **`./project/docs/analysis/python/Analysis_Report_{YYYYMMDD-HHMMSS}.ipynb`** - Comprehensive analysis report and findings
2. **`./project/docs/analysis/python/Findings_Recommendations_{YYYYMMDD-HHMMSS}.ipynb`** - Findings Recommendations
3. **`./project/docs/analysis/python/Action_Plan_{YYYYMMDD-HHMMSS}.ipynb`** - Action Plan with detailed breakdown and timeline

   - **Master Architecture Assessment**: Complete codebase overview and structural analysis
   - **Module & Import Analysis**: Package organization, dependencies, and circular import detection
   - **Class & Function Analysis**: OOP structure, inheritance hierarchies, and complexity metrics
   - **Type Safety Assessment**: Type annotation coverage and consistency validation

2. **`Critical_Issues_Security_Assessment-YYYY-MM-DD-HHMMSS.ipynb`** - High-priority issues requiring immediate attention
   - **Security Vulnerabilities**: Critical security issues and vulnerability assessment
   - **Performance Bottlenecks**: Resource management patterns and memory handling issues
   - **Exception Handling**: Error propagation patterns and exception management problems
   - **Anti-Pattern Detection**: Code smells and architectural violations

3. **`Improvement_Roadmap_Recommendations-YYYY-MM-DD-HHMMSS.ipynb`** - Actionable steps for code enhancement
   - **Python Best Practices**: Pythonic code evaluation and improvement suggestions
   - **Async & Concurrency**: Async patterns, threading issues, and optimization opportunities
   - **Refactoring Priorities**: Ranked list of refactoring targets with impact assessment
   - **Modernization Path**: Python version migration and modern pattern adoption

**NEXT PHASE PREPARATION:**

```bash
# After analysis completion, execute improvements with:
/python-code-refactor [analysis-results] [refactor-scope]
/python-code-optimize [analysis-results] [optimization-focus]
/python-code-modernize [analysis-results] [python-version]

# Examples:
/python-code-refactor Python_Code_Analysis_Overview-2025-09-22-085549.ipynb module-structure
/python-code-optimize Function_Complexity_Report-2025-09-22-085549.ipynb performance
/python-code-modernize Type_Annotation_Coverage-2025-09-22-085549.ipynb python-3.11
```

---

**ENFORCEMENT:** This command performs COMPREHENSIVE PYTHON CODE ANALYSIS through the MCP prompt protocol. The comprehensive analysis logic is defined in `python-code-analysis-prompt.yaml` and executed according to Model Context Protocol standards. Use subsequent Python refactoring commands for remediation execution after analysis is complete and approved.

## Python Analysis Framework

### **PYTHON-SPECIFIC ANALYSIS TOOLS:**

1. **Static Analysis:**
   - `ast` module for Abstract Syntax Tree analysis
   - `dis` module for bytecode examination
   - `inspect` module for live object introspection
   - `importlib` for dynamic import analysis

2. **Complexity Metrics:**
   - McCabe cyclomatic complexity
   - Halstead complexity measures
   - Cognitive complexity scoring
   - Maintainability index calculation

3. **Dependency Analysis:**
   - `pipdeptree` for package dependencies
   - `pydeps` for module dependency graphs
   - `import-linter` for import rules enforcement
   - Custom AST visitors for circular detection

4. **Type Analysis:**
   - `mypy` AST for type annotation coverage
   - `typing_extensions` for advanced types
   - Runtime type checking patterns
   - Protocol and ABC usage analysis

5. **Performance Profiling:**
   - `cProfile` integration points
   - `memory_profiler` hooks
   - `line_profiler` instrumentation
   - Async profiling with `aiomonitor`

### Phase 1: Component Inventory
Create a complete inventory of all system components:

1. **Applications & Entry Points**
   - Enumerate all executable scripts
   - Identify all CLI tools in `/tools/`
   - Map all test executables in `/tests/`
   - Document all utility scripts

2. **Service Architecture**
   - List all services in `/src/services/`
   - Map service dependencies
   - Document service interfaces
   - Identify service contracts

3. **Modules & Libraries**
   - Catalog all Python modules (.py)
   - Map module exports and imports
   - Document helper functions
   - Identify shared libraries

4. **Data & Configuration**
   - List all configuration files
   - Map data schemas
   - Document credential stores
   - Identify environment configs

5. **Infrastructure Components**
   - Document API integrations
   - Map network communication
   - List external dependencies
   - Identify monitoring hooks

<examples>
  <component_inventory_analysis>
    <inventory_process>
      **Python Module Discovery:**
      ```python
      # AST-based module analysis
      import ast
      import os
      from pathlib import Path
      
      class ModuleInventory:
          def __init__(self, root_path: str):
              self.root_path = Path(root_path)
              self.modules = []
              self.entry_points = []
              self.test_files = []
          
          def discover_modules(self):
              for py_file in self.root_path.rglob("*.py"):
                  try:
                      with open(py_file, 'r') as f:
                          tree = ast.parse(f.read())
                      
                      module_info = {
                          'path': py_file,
                          'imports': self._extract_imports(tree),
                          'classes': self._extract_classes(tree),
                          'functions': self._extract_functions(tree),
                          'complexity': self._calculate_complexity(tree)
                      }
                      self.modules.append(module_info)
                  except SyntaxError:
                      # Document parsing failures
                      pass
      ```
      
      **Service Architecture Mapping:**
      - FastAPI services: Identify router modules and dependency injection
      - Django services: Map apps, models, and view hierarchies
      - Data Science: Identify pipeline modules and ML model definitions
      - Shared utilities: Common libraries and helper functions
    </inventory_process>
  </component_inventory_analysis>

  <dependency_mapping_example>
    <dependency_visualization>
      **Import Dependency Analysis:**
      ```python
      # Network analysis of module dependencies
      import networkx as nx
      from collections import defaultdict
      
      class DependencyAnalyzer:
          def __init__(self):
              self.dependency_graph = nx.DiGraph()
              self.circular_imports = []
          
          def build_dependency_graph(self, modules):
              for module in modules:
                  module_name = self._get_module_name(module['path'])
                  for import_info in module['imports']:
                      if self._is_local_import(import_info):
                          self.dependency_graph.add_edge(
                              module_name, import_info['module']
                          )
          
          def detect_circular_imports(self):
              try:
                  cycles = list(nx.simple_cycles(self.dependency_graph))
                  self.circular_imports = cycles
                  return cycles
              except nx.NetworkXError:
                  return []
      ```
      
      **Visualization Output:**
      - Dependency network graphs with Graphviz
      - Circular import detection with cycle highlighting
      - Module coupling metrics and cohesion analysis
      - Critical path analysis for refactoring priorities
    </dependency_visualization>
  </dependency_mapping_example>
</examples>

### Phase 2: SOLID Principles Analysis

<solid_principles_analysis>
  <single_responsibility>
    <analysis_points>
      <point>Count responsibilities for each class/module</point>
      <point>Measure cohesion</point>
      <point>Identify violation patterns</point>
      <point>Suggest refactoring opportunities</point>
    </analysis_points>
  </single_responsibility>

  <open_closed>
    <analysis_points>
      <point>Check for hardcoded logic</point>
      <point>Evaluate configuration flexibility</point>
      <point>Identify extension points</point>
      <point>Review modification risks</point>
    </analysis_points>
  </open_closed>

  <liskov_substitution>
    <analysis_points>
      <point>Verify contract compliance</point>
      <point>Check behavioral consistency</point>
      <point>Identify type violations</point>
      <point>Analyze polymorphism usage</point>
    </analysis_points>
  </liskov_substitution>

  <interface_segregation>
    <analysis_points>
      <point>Check interface granularity</point>
      <point>Identify fat interfaces</point>
      <point>Review client dependencies</point>
      <point>Suggest interface splitting</point>
    </analysis_points>
  </interface_segregation>

  <dependency_inversion>
    <analysis_points>
      <point>Map concrete dependencies</point>
      <point>Check abstraction usage</point>
      <point>Identify tight coupling</point>
      <point>Review injection patterns</point>
    </analysis_points>
  </dependency_inversion>
</solid_principles_analysis>

<examples>
  <solid_analysis_implementation>
    <single_responsibility_check>
      **Python Class Responsibility Analysis:**
      ```python
      class ResponsibilityAnalyzer:
          def analyze_class(self, class_node: ast.ClassDef) -> dict:
              responsibilities = {
                  'data_access': self._has_database_operations(class_node),
                  'business_logic': self._has_business_rules(class_node),
                  'presentation': self._has_presentation_logic(class_node),
                  'validation': self._has_validation_logic(class_node),
                  'external_communication': self._has_api_calls(class_node)
              }
              
              responsibility_count = sum(responsibilities.values())
              violation_severity = 'high' if responsibility_count > 2 else 'low'
              
              return {
                  'class_name': class_node.name,
                  'responsibilities': responsibilities,
                  'count': responsibility_count,
                  'violation': responsibility_count > 1,
                  'severity': violation_severity,
                  'refactoring_suggestions': self._suggest_refactoring(responsibilities)
              }
      ```
      
      **Analysis Output:**
      - Classes with multiple responsibilities identified
      - Refactoring suggestions with separation strategies
      - Cohesion metrics and improvement recommendations
    </single_responsibility_check>

    <dependency_inversion_analysis>
      **Abstraction vs Concretion Analysis:**
      ```python
      class DependencyInversionAnalyzer:
          def analyze_dependencies(self, module_node: ast.Module) -> dict:
              concrete_deps = []
              abstract_deps = []
              
              for node in ast.walk(module_node):
                  if isinstance(node, ast.Import):
                      for alias in node.names:
                          if self._is_concrete_dependency(alias.name):
                              concrete_deps.append(alias.name)
                          elif self._is_abstraction(alias.name):
                              abstract_deps.append(alias.name)
              
              dependency_score = len(abstract_deps) / (len(concrete_deps) + len(abstract_deps)) if (concrete_deps or abstract_deps) else 0
              
              return {
                  'concrete_dependencies': concrete_deps,
                  'abstract_dependencies': abstract_deps,
                  'inversion_score': dependency_score,
                  'violations': [dep for dep in concrete_deps if self._is_violation(dep)],
                  'recommendations': self._generate_di_recommendations(concrete_deps)
              }
      ```
      
      **Dependency Injection Pattern Detection:**
      - Constructor injection usage analysis
      - Service locator anti-pattern detection
      - Abstract base class and Protocol usage
      - Dependency injection framework integration
    </dependency_inversion_analysis>
  </solid_analysis_implementation>

  <comprehensive_solid_report>
    <analysis_output>
      **SOLID Principles Compliance Report:**
      
      **Single Responsibility Principle (SRP):**
      - Classes analyzed: 127
      - Violations detected: 23 (18%)
      - High severity violations: 8
      - Refactoring candidates: UserService, OrderManager, DataProcessor
      
      **Open/Closed Principle (OCP):**
      - Extension points identified: 15
      - Hardcoded logic instances: 31
      - Configuration flexibility score: 72%
      - Recommended abstractions: PaymentProcessor, NotificationService
      
      **Liskov Substitution Principle (LSP):**
      - Inheritance hierarchies: 12
      - Contract violations: 3
      - Polymorphism usage: 89%
      - Type safety issues: 2
      
      **Interface Segregation Principle (ISP):**
      - Interfaces analyzed: 34
      - Fat interfaces detected: 5
      - Client dependency mapping complete
      - Interface splitting recommendations: DatabaseRepository, EventHandler
      
      **Dependency Inversion Principle (DIP):**
      - Abstraction usage: 78%
      - Concrete dependency violations: 19
      - Injection pattern usage: 65%
      - Framework integration opportunities: 12
    </analysis_output>
  </comprehensive_solid_report>
</examples>

### Phase 3: Design Patterns Analysis

1. **Creational Patterns**
   - Factory pattern usage
   - Singleton implementation review
   - Builder pattern identification
   - Prototype pattern analysis

2. **Structural Patterns**
   - Adapter pattern usage
   - Facade pattern implementation
   - Decorator pattern review
   - Proxy pattern analysis

3. **Behavioral Patterns**
   - Strategy pattern usage
   - Observer pattern implementation
   - Command pattern review
   - Template method analysis

### Phase 4: Anti-Pattern Detection

1. **Code Smells**
   - Long methods (>50 lines)
   - Large classes (>500 lines)
   - Feature envy
   - Data clumps
   - Primitive obsession
   - Switch statements abuse
   - Parallel inheritance hierarchies
   - Lazy classes
   - Middle man
   - Message chains

2. **Design Anti-Patterns**
   - God objects/classes
   - Spaghetti code
   - Copy-paste programming
   - Magic numbers/strings
   - Hard coding
   - Premature optimization
   - Over-engineering
   - Under-abstraction
   - Circular dependencies
   - Tight coupling

3. **PowerShell-Specific Anti-Patterns**
   - Pipeline abuse
   - Inefficient filtering
   - Missing error handling
   - Improper scope usage
   - Synchronous-only operations
   - Missing parameter validation
   - Unsafe type conversions
   - Global variable pollution

### Phase 5: Clean Code Principles

1. **Naming Conventions**
   - Function naming clarity
   - Variable naming consistency
   - Parameter naming standards
   - Constant naming patterns

2. **Function Quality**
   - Function length analysis
   - Parameter count review
   - Side effect detection
   - Return type consistency

3. **Code Organization**
   - Module cohesion
   - Logical grouping
   - Dependency organization
   - File structure standards

4. **Comments & Documentation**
   - Comment quality
   - Documentation completeness
   - Code self-documentation
   - Outdated comment detection

### Phase 6: Security Best Practices

1. **Input Validation**
   - Parameter validation coverage
   - Type checking implementation
   - Boundary condition handling
   - Injection prevention

2. **Authentication & Authorization**
   - Credential handling review
   - Permission checking
   - Session management
   - Access control patterns

3. **Cryptography**
   - Encryption usage
   - Key management
   - Hash function selection
   - Random number generation

4. **Error Handling**
   - Exception management
   - Error message exposure
   - Logging practices
   - Fail-safe defaults

### Phase 7: Performance Patterns

1. **Resource Management**
   - Memory usage patterns
   - Connection pooling
   - Object disposal
   - Cache implementation

2. **Algorithmic Efficiency**
   - Time complexity analysis
   - Space complexity review
   - Optimization opportunities
   - Bottleneck identification

### Phase 8: Testing & Maintainability

1. **Testability**
   - Test coverage analysis
   - Mockability assessment
   - Dependency injection usage
   - Test isolation review

2. **Maintainability Metrics**
   - Cyclomatic complexity
   - Coupling metrics
   - Cohesion analysis
   - Change risk assessment

<examples>
  <testing_analysis_implementation>
    <coverage_analysis>
      **Test Coverage Assessment:**
      ```python
      import coverage
      import pytest
      from pathlib import Path
      
      class TestabilityAnalyzer:
          def __init__(self, project_root: str):
              self.project_root = Path(project_root)
              self.cov = coverage.Coverage()
              
          def analyze_test_coverage(self):
              # Run tests with coverage
              self.cov.start()
              pytest.main([str(self.project_root / "tests"), "-v"])
              self.cov.stop()
              self.cov.save()
              
              # Generate coverage report
              coverage_data = self.cov.get_data()
              report_data = {}
              
              for filename in coverage_data.measured_files():
                  analysis = self.cov.analysis2(filename)
                  report_data[filename] = {
                      'statements': len(analysis[1]),
                      'missing': len(analysis[3]),
                      'coverage_percent': (1 - len(analysis[3]) / len(analysis[1])) * 100 if analysis[1] else 0,
                      'missing_lines': analysis[3]
                  }
              
              return report_data
      ```
      
      **Mockability Assessment:**
      - Dependency injection pattern detection
      - Hard dependencies identification
      - External service integration points
      - Database coupling analysis
    </coverage_analysis>

    <maintainability_metrics>
      **Complexity and Maintainability Analysis:**
      ```python
      import ast
      from radon.complexity import cc_visit
      from radon.metrics import mi_visit, h_visit
      
      class MaintainabilityAnalyzer:
          def analyze_module_maintainability(self, module_path: str) -> dict:
              with open(module_path, 'r') as f:
                  source = f.read()
              
              tree = ast.parse(source)
              
              # Cyclomatic complexity
              complexity_blocks = cc_visit(source)
              avg_complexity = sum(block.complexity for block in complexity_blocks) / len(complexity_blocks) if complexity_blocks else 0
              
              # Maintainability index
              mi_score = mi_visit(source, multi=True)
              
              # Halstead metrics
              halstead = h_visit(source)
              
              return {
                  'cyclomatic_complexity': {
                      'average': avg_complexity,
                      'max': max(block.complexity for block in complexity_blocks) if complexity_blocks else 0,
                      'high_complexity_functions': [b.name for b in complexity_blocks if b.complexity > 10]
                  },
                  'maintainability_index': mi_score,
                  'halstead_metrics': {
                      'difficulty': halstead.difficulty,
                      'effort': halstead.effort,
                      'volume': halstead.volume
                  },
                  'change_risk': self._assess_change_risk(complexity_blocks, mi_score)
              }
      ```
      
      **Change Risk Assessment:**
      - High complexity function identification
      - Coupling analysis between modules
      - Code churn correlation with bug reports
      - Refactoring priority scoring
    </maintainability_metrics>
  </testing_analysis_implementation>

  <comprehensive_quality_report>
    <quality_metrics_summary>
      **Code Quality and Maintainability Summary:**
      
      **Test Coverage Analysis:**
      - Overall coverage: 87%
      - Critical path coverage: 94%
      - Untested modules: 8
      - Missing test scenarios: Authentication edge cases, payment failure handling
      
      **Complexity Metrics:**
      - Average cyclomatic complexity: 6.2
      - Functions exceeding complexity threshold (>10): 12
      - Highest complexity function: process_order_workflow (CC: 23)
      - Maintainability index: 74 (Good)
      
      **Coupling and Cohesion:**
      - Afferent coupling (Ca): 15.3 average
      - Efferent coupling (Ce): 8.7 average
      - Instability (I): 0.36
      - Cohesion score: 82%
      
      **Change Risk Assessment:**
      - High risk modules: 5
      - Medium risk modules: 12
      - Refactoring priority: OrderService, PaymentProcessor, UserManager
      - Technical debt estimate: 2.3 weeks
    </quality_metrics_summary>
  </comprehensive_quality_report>
</examples>

## Quality Metrics & Scoring

### Code Quality Score (0-100)
```
Components:
- SOLID Compliance: 20%
- Pattern Usage: 15%
- Anti-Pattern Absence: 20%
- Clean Code: 15%
- Security: 15%
- Performance: 10%
- Maintainability: 5%
```

### Severity Classification
- **Critical**: Security vulnerabilities, data loss risks
- **High**: Major anti-patterns, SOLID violations
- **Medium**: Code smells, minor pattern issues
- **Low**: Style inconsistencies, optimization opportunities


## Execution Command

```powershell
# Perform comprehensive code quality analysis
.\tools\PerformCodeQualityAnalysis.ps1 `
    -Scope "Full" `
    -AnalysisDepth "Comprehensive" `
    -IncludePatterns `
    -IncludeAntiPatterns `
    -IncludeSecurity `
    -IncludePerformance `
    -GenerateVisuals `
    -OutputFormat @("HTML", "JSON", "PDF")
```

## Quality Checkpoints

Ensure complete analysis coverage:
- [ ] All functions analyzed for SOLID compliance
- [ ] All modules checked for patterns
- [ ] All code blocks scanned for anti-patterns
- [ ] All inputs validated for security
- [ ] All algorithms reviewed for efficiency
- [ ] All dependencies mapped
- [ ] All complexity calculated
- [ ] All test coverage measured
- [ ] All documentation reviewed
- [ ] All metrics generated

<examples>
  <quality_checkpoint_workflow>
    <checkpoint_validation>
      **Quality Analysis Validation Process:**
      
      **Phase 1 - Structural Analysis Completion**
       Module inventory: 127 Python files catalogued
       Import analysis: Dependency graph generated with 23 circular imports detected
       Class hierarchy: 45 classes mapped, 12 inheritance chains analyzed
       Function analysis: 342 functions assessed for complexity and cohesion
      
      **Phase 2 - SOLID Principles Compliance**
       Single Responsibility: 23 violations identified with refactoring suggestions
       Open/Closed: 15 extension points mapped, 31 hardcoded logic instances
       Liskov Substitution: 3 contract violations in inheritance hierarchies
       Interface Segregation: 5 fat interfaces detected with splitting recommendations
       Dependency Inversion: 78% abstraction usage, 19 concrete dependency violations
      
      **Phase 3 - Quality and Security Assessment**
       Security scan: 4 vulnerabilities detected (2 high, 2 medium priority)
       Performance analysis: 8 bottlenecks identified in async operations
       Type checking: 89% type annotation coverage, 23 mypy errors
       Test coverage: 87% overall, 12 critical paths missing tests
      
      **Phase 4 - Anti-Pattern Detection**
       Code smells: 34 instances of long methods, 8 god classes
       Python-specific issues: 12 synchronous operations in async contexts
       Design anti-patterns: 6 singletons, 4 circular dependencies
       Performance anti-patterns: 15 N+1 query instances, 7 memory leaks
      
      **Final Validation**: All quality checkpoints completed with comprehensive findings
    </checkpoint_validation>
  </quality_checkpoint_workflow>

  <analysis_completion_verification>
    <verification_process>
      **Comprehensive Analysis Verification:**
      
      **Deliverable Validation:**
      -  Comprehensive_Code_Analysis_Report-2025-09-25-143000.ipynb created
      -  Critical_Issues_Security_Assessment-2025-09-25-143000.ipynb generated
      -  Improvement_Roadmap_Recommendations-2025-09-25-143000.ipynb produced
      -  All notebooks contain proper timestamps and analysis metadata
      
      **Analysis Coverage Verification:**
      -  100% of Python files processed (excluding syntax error files)
      -  All major frameworks detected and analyzed (FastAPI, SQLAlchemy, Pydantic)
      -  Security assessment covers all vulnerability categories
      -  Performance analysis includes async/await patterns and database operations
      
      **Quality Standards Met:**
      -  Analysis depth: Comprehensive (all 8 phases completed)
      -  Documentation: All findings documented with examples and recommendations
      -  Prioritization: Issues ranked by severity and business impact
      -  Actionability: All recommendations include implementation steps
      
      **Ready for Next Phase**: Analysis complete, ready for improvement implementation
    </verification_process>
  </analysis_completion_verification>
</examples>